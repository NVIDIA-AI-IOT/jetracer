{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apex Tracking - Data Collection\n",
    "\n",
    "In this notebook, we'll collect a dataset that will be used to steer JetRacer.  To do this, we'll manually place JetRacer in different positions and orientations along the track, and collect a dataset of x, y coordinates that correspond to a target point that JetRacer should steer torwards.  Taking some liberties, we'll call this target point the *apex*.\n",
    "\n",
    "To collect this dataset, follow these steps\n",
    "\n",
    "1.  Place JetRacer in a random position / orientation on the track\n",
    "\n",
    "2.  Imagine the optimal path that JetRacer would follow\n",
    "\n",
    "3.  Click as far along that imaginary path such that JetRacer could travel straight there without running out of bounds (or hitting an object, etc.)\n",
    "\n",
    "4.  Repeat 1-3\n",
    "\n",
    "We've been able to get a decent working neural network using about 100 images following this procedure.  If you're familiar with our previous project, *JetBot*, you'll know that it's not only important to have a good amount of images, but well varied images.\n",
    "\n",
    "Make sure you cover different spots on the track, different offsets from the centerline, and if you can vary other parameters like brightness.\n",
    "\n",
    "After we've collected a dataset to test, we'll open up the *train_model.ipynb* notebook to train our neural network.  \n",
    "\n",
    "So let's get started.  First, we'll create and display a *ClickableImageWidget*, which will be used to annotate the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from jupyter_clickable_image_widget import ClickableImageWidget\n",
    "\n",
    "preview_widget = ClickableImageWidget(format='jpeg', width=224, height=224)\n",
    "\n",
    "display(preview_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, you won't see any image displayed.  We need to initialize and connect the camera to the widget. Let's start our camera and connect it to the clickable image widget using the *traitlets* library.\n",
    "\n",
    "The IMX219 camera driver on Jetson supports up to 120FPS at certain capture resolutions.  One of these capture resolutions is 1280x720, so we will manually set that.  Our output resolution will be 224x224, which will be fed directly to the neural network. Since we're only collecting data, we will reduce the framerate to just 10FPS to limit network streaming bandwidth.\n",
    "\n",
    "We set the ``camera.running = True`` so that the camera's internal ``value`` will be set automatically.  This allows us to attach callbacks to it which will automatically get called when new camera frames arrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traitlets\n",
    "from jetcam.csi_camera import CSICamera\n",
    "from jetcam.utils import bgr8_to_jpeg\n",
    "\n",
    "camera = CSICamera(width=224, height=224, capture_width=1280, capture_height=720, capture_fps=10)\n",
    "\n",
    "camera.running = True\n",
    "\n",
    "camera_link = traitlets.dlink((camera, 'value'), (preview_widget, 'value'), transform=bgr8_to_jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create another image widget that will be used to display the latest snapshot sample that we add to our dataset.\n",
    "\n",
    "We'll also create a widget to display the number of images in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets\n",
    "\n",
    "snapshot_widget = ipywidgets.Image(format='jpeg', width=224, height=224)\n",
    "count_widget = ipywidgets.IntText(description='count')\n",
    "\n",
    "display(snapshot_widget, count_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This won't display anything yet.  We need to create the dataset class, and attach a callback to the preview *ClickableImageWidget* to save the clicked x, y coordinates.\n",
    "\n",
    "We've created two python files in the ``apex_tracking`` example folder named ``xy_dataset.py`` and ``utils.py``.  ``xy_dataset.py`` defines a class that makes it easy for us to save / load annotations, which we'll also use for training.  Under the hood, this dataset just dumps images in a folder with the name ``<x_coordinate>_<y_coordinate>_<uuid>.jpg``.  Where ``uuid`` is a unique string that is generated using the Python ``uuid`` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xy_dataset import XYDataset\n",
    "\n",
    "dataset = XYDataset('apex_dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add (image, x, y) tuples to the dataset by calling the ``save_entry`` method.  We'll do this in a callback that we attach to the ClickableImageWidget below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def save_snapshot(_, content, msg):\n",
    "    global camera, dataset, snapshot_widget\n",
    "    if content['event'] == 'click':\n",
    "        data = content['eventData']\n",
    "        x = data['offsetX']\n",
    "        y = data['offsetY']\n",
    "        \n",
    "        # save to disk\n",
    "        dataset.save_entry(camera.value, x, y)\n",
    "        \n",
    "        # draw circle on snapshot\n",
    "        snapshot = camera.value.copy()\n",
    "        snapshot = cv2.circle(snapshot, (x, y), 8, (0, 255, 0), 3)\n",
    "    \n",
    "        snapshot_widget.value = bgr8_to_jpeg(snapshot)\n",
    "        count_widget.value = len(dataset)\n",
    "        \n",
    "preview_widget.on_msg(save_snapshot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we click the preview widget, we'll see an image drawn on the snapshot widget with a green circle.  We should also see the image count increase.\n",
    "\n",
    "To make it easier, let's display all of the widgets below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    ipywidgets.HBox([preview_widget, snapshot_widget]),\n",
    "    count_widget\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now go ahead and place JetRacer in different spots on the track and collect a dataset of at least 100 images as described earlier!\n",
    "\n",
    "Remember to vary the placement of the racecar for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -rq apex_dataset.zip apex_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next\n",
    "\n",
    "Follow the train_model.ipynb to train the neural network to predict the Apex from an image!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
